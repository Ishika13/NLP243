{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSTyzj9gC4qR",
        "outputId": "9d8c8111-e153-4156-d28f-bfb694a17da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7570036641425557\n",
            "Epoch 2, Loss: 0.7685506749484274\n",
            "Epoch 3, Loss: 0.4977883601354228\n",
            "Epoch 4, Loss: 0.3466450195345614\n",
            "Epoch 5, Loss: 0.2507910542190075\n",
            "Epoch 6, Loss: 0.19179138779226276\n",
            "Epoch 7, Loss: 0.15101299481466413\n",
            "Epoch 8, Loss: 0.12378427701898748\n",
            "Epoch 9, Loss: 0.10121867852285504\n",
            "Epoch 10, Loss: 0.08482914655986759\n",
            "F1 Score: 0.276\n",
            "Submission file saved as 'slot_tagging_submission.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load data\n",
        "def load_data():\n",
        "    train_df = pd.read_csv('/content/hw2_train.csv')\n",
        "    test_df = pd.read_csv('/content/hw2_test.csv')\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_data()\n",
        "train_utterances = train_df['utterances'].apply(lambda x: x.split()).tolist()\n",
        "test_utterances = test_df['utterances'].apply(lambda x: x.split()).tolist()\n",
        "train_tags = [tags.split() for tags in train_df['IOB Slot tags'].tolist()]\n",
        "\n",
        "# Create vocabularies\n",
        "def create_vocab(utterances, tags):\n",
        "    token_vocab = {word: idx for idx, word in enumerate(set([word for utterance in utterances for word in utterance]))}\n",
        "    tag_vocab = {tag: idx for idx, tag in enumerate(set([tag for tag_seq in tags for tag in tag_seq]))}\n",
        "    return token_vocab, tag_vocab\n",
        "\n",
        "token_vocab, tag_vocab = create_vocab(train_utterances, train_tags)\n",
        "\n",
        "# Dataset class\n",
        "class SlotTaggingDataset(Dataset):\n",
        "    def __init__(self, utterances, tags, token_vocab, tag_vocab):\n",
        "        self.utterances = utterances\n",
        "        self.tags = tags\n",
        "        self.token_vocab = token_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        utterance = self.utterances[idx]\n",
        "        tag = self.tags[idx]\n",
        "        input_tensor = torch.tensor([self.token_vocab.get(token, 0) for token in utterance], dtype=torch.long)\n",
        "        target_tensor = torch.tensor([self.tag_vocab.get(t, 0) for t in tag], dtype=torch.long)\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "# Padding function\n",
        "def collate_fn(batch):\n",
        "    input_seqs, target_seqs = zip(*batch)\n",
        "    input_seqs_padded = pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
        "    target_seqs_padded = pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
        "    return input_seqs_padded, target_seqs_padded\n",
        "\n",
        "# Model definition\n",
        "class SlotTaggingLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.1):\n",
        "        super(SlotTaggingLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# Initialize model\n",
        "def initialize_model(input_dim, output_dim):\n",
        "    model = SlotTaggingLSTM(input_dim, hidden_dim=128, output_dim=output_dim, num_layers=1, dropout=0.1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    return model, criterion, optimizer, device\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for input_tensor, target_tensor in train_loader:\n",
        "            input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            output = output.view(-1, len(tag_vocab))\n",
        "            target_tensor = target_tensor.view(-1)\n",
        "\n",
        "            loss = criterion(output, target_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    for input_tensor, true_tags in test_loader:\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        true_tags = true_tags.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "\n",
        "        pred_tags = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "        true_tags = true_tags.cpu().numpy()\n",
        "\n",
        "        all_preds.extend(pred_tags.flatten())\n",
        "        all_true.extend(true_tags.flatten())\n",
        "\n",
        "    f1 = f1_score(all_true, all_preds, average='weighted')\n",
        "    print(f\"F1 Score: {f1:.3f}\")\n",
        "    return all_preds\n",
        "\n",
        "# Prepare submission\n",
        "def prepare_submission(all_preds, test_utterances):\n",
        "    all_preds_reshaped = []\n",
        "    start_idx = 0\n",
        "    for i, utterance in enumerate(test_utterances):\n",
        "        seq_len = len(utterance)\n",
        "        all_preds_reshaped.append(all_preds[start_idx:start_idx + seq_len])\n",
        "        start_idx += seq_len\n",
        "\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'IOB Slot Tags': [' '.join([list(tag_vocab.keys())[pred] for pred in pred_seq]) for pred_seq in all_preds_reshaped]\n",
        "    })\n",
        "\n",
        "    submission_df.to_csv('/content/slot_tagging_submission.csv', index=False)\n",
        "    print(\"Submission file saved as 'slot_tagging_submission.csv'\")\n",
        "\n",
        "# Main code\n",
        "train_dataset = SlotTaggingDataset(train_utterances, train_tags, token_vocab, tag_vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model, criterion, optimizer, device = initialize_model(len(token_vocab), len(tag_vocab))\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)\n",
        "\n",
        "test_tags = [['O'] * len(utterance) for utterance in test_utterances]\n",
        "test_dataset = SlotTaggingDataset(test_utterances, test_tags, token_vocab, tag_vocab)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "all_preds = evaluate_model(model, test_loader, device)\n",
        "\n",
        "prepare_submission(all_preds, test_utterances)\n"
      ]
    }
  ]
}